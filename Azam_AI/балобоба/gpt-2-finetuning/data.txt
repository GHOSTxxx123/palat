Немного предыстории

Для тех кто не следил за развитием прогресса в обработке естественной речи (NLP).


Летом 2018 года OpenAI предобучили на большом объеме текста нейронную сеть GPT, построенную на архитектуре Transformer. Оказалось, что если заменить пару последних слоев и дообучить ее под конкретную задачу (такой подход называется Fine Tuning и широко используется в машинном обучении), то это бьет предыдущие рекорды сразу по широкому спектру разговорных задач.


На основе этой разработки, в конце 2018 года в Google создали свою нейросеть BERT. Они серьезно улучшили результат, сделав нейросеть двунаправленной, в отличие от GPT.


Не желая сдаваться, в феврале 2019 года в OpenAI увеличили свою GPT сразу в 10 раз и обучили ее на еще большем объеме текста — на 8 млн интернет страницах (суммарно на 40 Гб текста). Получившаяся таким образом сеть GPT-2 является на данный момент самой большой нейросетью, с беспрецендентным числом параметров 1.5 млрд (у BERT в самой крупной модели было 340 млн, а у стандартной BERT 110 млн).


Как результат, GPT-2 оказалась способной генерировать целые страницы связного текста. С повторными упоминаниями имен действующих лиц по ходу повествования, цитатами, отсылками к связанным событиям и так далее. Примеры приводить здесь не буду, а отсылаю желающих к оригинальной статье в блоге OpenAI: Better Language Models and Their Implications или по ссылкам в конце статьи.

